<?xml version="1.0" encoding="UTF-8"?>
<?oxygen RNGSchema="http://www.oasis-open.org/docbook/xml/5.0/rng/docbook.rng" type="xml"?>
<book xmlns="http://docbook.org/ns/docbook" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0">
    <info>
        <title>Boost Asynchronous</title>
        <author>
            <personname>Christophe Henry</personname>
            <email>christophe.j.henry@googlemail.com</email>
        </author>
        <copyright>
            <year>20013</year>
            <holder>
                <phrase> Distributed under the Boost Software License, Version 1.0. (See
                    accompanying file LICENSE_1_0.txt or copy at <link
                        xlink:href="http://www.boost.org/LICENSE_1_0.txt"
                        >http://www.boost.org/LICENSE_1_0.txt</link> ) </phrase>
            </holder>
        </copyright>
    </info>
    <preface>
        <title>Preface</title>
        <para>
            <emphasis role="underline">Note</emphasis>: Asynchronous is not part of the Boost
            library. It is planed to be offered for Review in 2014. At the moment it is still in
            development.</para>
        <para>Herb Sutter wrote in an article <command
                xlink:href="http://www.gotw.ca/publications/concurrency-ddj.htm"/> "The Free Lunch
            Is Over", meaning that developpers will be forced to learn to develop multi-threaded
            applications. This, however, brings a fundamental issue: multithreading is hard, it's
            full of ugly beasts waiting hidden for our mistakes: races, deadlocks, all kinds of
            subtle bugs. Worse yet, these bugs are hard to find because they are never reproducible
            when you are looking for them, which leaves us with backtrace analysis, and this is when
            we are lucky enough to have a backtrace in the first place.</para>
        <para>This is not even the only danger. CPUs are a magnitude faster than memory, I/O
            operations, network communications, which all stall our programms and degrade our
            performance, which means long sessions with coverage or analysis tools.</para>
        <para>Well, maybe the free lunch is not completely over yet, or at least maybe we can still
            get one a bit longer for a bargain. This is what Boost Asynchronous is helping
            solve.</para>
        <para>Boost Asynchronous is a library making it easy to write asynchronous code similar to
            the Proactor<command xlink:href="http://www.cs.wustl.edu/~schmidt/PDF/proactor.pdf"/>
            pattern. To achieve this, it offers tools for asynchronous designs: ActiveObject,
            threadpools, servants, proxies, queues, algorithms, etc. </para>
        <para>Asynchronous programming has the advantage of making it easier to design your code
            nonblocking, single-threaded while still getting your cores to work at full capacity.
            And all this while forgetting what a mutex is. Incorrect mutex usage is a huge source of
            bugs and Asynchronous helps you avoid them.</para>
        <para>However, the goal of this library is not to help you write massively parallel code.
            There are other solutions for this. This library is for the other 99% of us who happen
            to work on 4-12 cores hardware because 1000 cores are not affordable, but would still
            like to get the best of it while avoiding ugly bugs, get better diagnostic of what our
            application is doing and planing ourselves what our cores are doing. Asynchronous is not:<itemizedlist>
                <listitem>
                    <para>std/boost::async: both are asynchronous in a very limited way. One posts
                        asynchronously some work, but then? Well, then the choice is between
                        blocking for the future result (taboo) or polling from time to time, as in
                        the (bad) old times. Furthermore, one has no control on the
                        scheduler.</para>
                </listitem>
                <listitem>
                    <para>Intel TBB: this is a wonderful parallel library. But it's not asynchronous
                        as one needs to wait for the end of a parallel_ call. Sure, you have
                        pipelines, but when your application is complex, good luck to understand the
                        code. Again, one has limited control on the scheduler.</para>
                </listitem>
                <listitem>
                    <para>N3428: this is an interesting approach as it was at least recognized that
                        std::async is not asynchronous. So now, you get a kind of state machine
                        hidden behind a .then, when_any, when_all, which are a poor man's state
                        machines, besides ugly limitations like finding out which when_all threw an
                        exception. When did we give up writing design diagrams to document and
                        understand our code later? When a 15+ states state machine with guards,
                        event deferring and control flow has to be written with .then, please don't
                        ask me to review the code.</para>
                </listitem>
            </itemizedlist></para>
        <para>Let's have a quick look at code using futures and .then (taken from N3428):</para>
        <para>
            <programlisting>future&lt;int> f1 = async([]() { return 123; });
future&lt;string> f2 = f1.then([](future&lt;int> f) {return f.get().to_string();}); // here .get() wonâ€™t block
f2.get(); // just a "small get" at the end?</programlisting>
        </para>
        <para> Saying that there is only a "small get" at the end is, for an application with
            real-time constraints, equivalent to saying at a lockfree conference something like
            "what is all the fuss about? Can't we just add a small lock at the end?". Just try
            it...</para>
        <para>This brings us to a central point of Asynchronous: if we build a system with strict
            real-time constraints, there is no such thing as a small get(). We need to be able to
            react to any event in the system in a timely manner. And we can't affort to have lots of
            functions potentially waiting too long everywhere in our code. Therefore, .then is only
            good for an application of a few hunderds of lines. What about using a timed_wait
            instead? Nope. Either we wait too long before handling an error (this just limits the
            amount of time we wate waiting, that's all), or we wait not enough and we poll. In any
            case, while waiting, our thread cannot react to other events.</para>
        <para>All these libraries also have the disadvantage of working with functions, not classes.
            But we, normal developers, do use classes. And we want them safe. And you can hardly
            make a class safe if you simply pass it to another thread. Consider the following
            example:</para>
        <para>
            <programlisting>class Bad : public boost::signals::trackable
{
   int foo();
};
boost::shared_ptr&lt;Bad> b;
future&lt;int> f = async([b](){return b->foo()});          </programlisting>
        </para>
        <para> Now you have the ugly problem of not knowing in which thread Bad will be destroyed.
            And as it's pretty hard to have a thread-safe destructor, you find yourself with a race
            condition in it. Maybe you'll find a solution for signals, but are you sure nobody is
            ever going to do something dangerous in the destructor? This clearly is not thinking in
            the future sense.</para>
        <para>Another particularity of Asynchronous is that it's not hiding anything: one gets to
            pick a pool for a particular application, choose how many threads are to be used, and
            the type of queue which best corresponds to the job to do, which allows finer
            tuning.</para>
        <para>An image being more worth than thousand words, the following story will explain in a
            few minutes what Asynchronous is about. Consider some fast-food restaurant:</para>
        <para><inlinemediaobject>
                <imageobject>
                    <imagedata fileref="pics/Proactor1.jpg"/>
                </imageobject>
            </inlinemediaobject>
        </para>
        <para>This restaurant has a single employee, Worker, delivers burgers through a burger queue
            and drinks. A Customer comes. Then another, who waits until the first customer is
            served.</para>
        <para><inlinemediaobject>
                <imageobject>
                    <imagedata fileref="pics/Proactor2.jpg"/>
                </imageobject>
            </inlinemediaobject></para>
        <para>To keep customers happy by reducing waiting time, the restaurant owner hires a second
            employee:</para>
        <para><inlinemediaobject>
                <imageobject>
                    <imagedata fileref="pics/Proactor3.jpg"/>
                </imageobject>
            </inlinemediaobject></para>
        <para>Unfortunately, this brings chaos in the restaurant. Sometimes, employes fight to get a
            burger to their own customer first:</para>
        <para><inlinemediaobject>
                <imageobject>
                    <imagedata fileref="pics/Proactor-RC.jpg"/>
                </imageobject>
            </inlinemediaobject></para>
        <para>And sometimes, they stay in each other's way:</para>
        <para><inlinemediaobject>
                <imageobject>
                    <imagedata fileref="pics/Proactor-DL.jpg"/>
                </imageobject>
            </inlinemediaobject></para>
        <para>This clearly is a not an optimal solution. Not only the additional employee brings
            additional costs, but both employees now spend much more time waiting. It also is not a
            scalable solution if even more customers want to eat because it's lunch-time right now.
            Even worse, as they fight for resources and stay in each other's way, the restaurant now
            serves people less fast than before. Customers flee and the restaurant gets bankrupt. A
            sad story, isn't it? To avoid this, the owner decides to go asynchronous. He keeps a
            single worker, who moves from cash desk to cash desk:</para>
        <para><inlinemediaobject>
                <imageobject>
                    <imagedata fileref="pics/Proactor-async.jpg"/>
                </imageobject>
            </inlinemediaobject></para>
        <para>The worker never waits because it would increase customer's waiting time. Instead, he
            runs from cash desks to the burger queue, beverage machine using a self-made strategy: <itemizedlist>
                <listitem>
                    <para>ask what the customer wants and keep an up-to-date information of the
                        customer's state.</para>
                </listitem>
                <listitem>
                    <para>if we have another customer at a desk, ask what he wants. For both
                        customers, remember the state of the order (waiting for customer choice,
                        getting food, getting drink, delivering, getting payment, etc.)</para>
                </listitem>
                <listitem>
                    <para>as soon as some new state is detected (customer choice, burger in the
                        queue, drink ready), handle it.</para>
                </listitem>
                <listitem>
                    <para>priorities are defined: start the longest-lasting tasks first, serve
                        angry-looking customers first, etc.</para>
                </listitem>
            </itemizedlist></para>
        <para>The following diagram shows us the busy worker in action:</para>
        <para><inlinemediaobject>
                <imageobject>
                    <imagedata fileref="pics/Proactor-async2.jpg"/>
                </imageobject>
            </inlinemediaobject></para>
        <para>Of course the owner needs a worker who runs fast, and has a pretty good memory so he
            can remember what customers are waiting for. </para>
        <para>This is what Asynchronous is for. A worker (thread) runs as long as there are waiting
            customers, following a precisely defined algorithm, and lots of state machines to manage
            the asynchronous behaviour. In case of customers, we could have a state machine: Waiting
            -> PickingMenu -> WaitingForFood -> Paying.</para>
        <para>We also need some queues (Burger queue, Beverage glass positioning) and some
            Asynchronous Operation Processor (for example a threadpool made of workers in the
            kitchen), event of different types (Drinks delivery). Maybe you also want some work
            stealing (someone in the kitchen serving drinks as he has no more burger to prepare. He
            will be slower than the machine, but still bring some time gain).</para>
        <para><emphasis role="bold">To make this work, the worker must not block, never,
                ever</emphasis>. And whatever he's doing has to be as fast as possible, otherwise
            the whole process stalls.</para>
    </preface>
    <part>
        <title>Concepts</title>
        <chapter>
            <title>Related designs: std::async, Active Object, Proactor</title>
            <sect1>
                <title>std::async</title>
                <subtitle>What is wrong with it</subtitle>
                <para>TODO</para>
            </sect1>
            <sect1>
                <title>Active Object</title>
                <subtitle>Design</subtitle>
                <para><inlinemediaobject>
                        <imageobject>
                            <imagedata fileref="pics/ActiveObject.jpg"/>
                        </imageobject>
                    </inlinemediaobject></para>
                <para>This simplified diagram shows a possible design variant of an Active Object
                    pattern.</para>
                <para>A thread-unsafe Servant is hidden behind a Proxy, which offers the same
                    members as the Servant itself. This Proxy is called by clients and delivers a
                    future object, which will, at some later point, contain the result of the
                    corresponding member called on the servant. The Proxy packs a MethodRequest
                    corresponding to a Servant call into the ActivationQueue. The Scheduler waits
                    permanently for MethodRequests in the queue, dequeues them, and executes them.
                    As only one scheduler waits for requests, it serializes access to the Servant,
                    thus providing thread-safety.</para>
                <para>However, this pattern presents some liabilities:<itemizedlist>
                        <listitem>
                            <para>Performance overhead: depending on the system, data moving and
                                context switching can be a performance drain.</para>
                        </listitem>
                        <listitem>
                            <para>Memory overhead: for every Servant, a thread has to be created,
                                consuming resources.</para>
                        </listitem>
                        <listitem>
                            <para>Usage: getting a future doesn't bring you as much asynchronous
                                behaviour as one might think. Usually, docs tell you to do something
                                else and check it later. But most cases simply mean that the client
                                will earlier or later block until the future is ready. This also
                                applies to std/boost::async.</para>
                        </listitem>
                    </itemizedlist></para>
            </sect1>
            <sect1>
                <title>Proactor</title>
                <subtitle>Design</subtitle>
                <para><inlinemediaobject>
                        <imageobject>
                            <imagedata fileref="pics/Proactor.jpg"/>
                        </imageobject>
                    </inlinemediaobject></para>
                <para>TODO</para>
            </sect1>
        </chapter>
        <chapter>
            <title>Features</title>
            <sect1>
                <title>Active Component</title>
                <subtitle>Extending Active Objects with more servants within a thread
                    context</subtitle>
                <para>A commonly cited drawback of Active Objects is that it's awfully expensive. A
                    thread per object is really a waste of ressources. Boost.Asynchronous extends
                    this concept by allowing an unlimited number of objects to live within a single
                    thread context, thus amortizing the costs.</para>
                <para>This brings another difference with ActiveObjects. As many objects are
                    potentially living in a thread context, none should be allowed to process
                    long-lasting tasks as it would reduce reactivity of the whole component. In this
                    aspect, Asynchronous' philosophy is closer to a Proactor.</para>
                <para>As long-lasting tasks do happen, Boost.Asynchronous provides several
                    implementations of threadpools and the infrastructure to make it safe to post
                    work to threadpools and get aynchronously a callback. It also provides safe
                    mechanisms to shutdown Active Components and threadpools.</para>
            </sect1>
            <sect1>
                <title>Shutting down</title>
                <para>Shutting down a thread turns out to be harder in practice than expected, as
                    shown by several posts of surprise when Boost.Thread tried to match the C++
                    Standard. Asynchronous hides all these ugly details. What users see is a proxy
                    object, which can be shared by any number of objects executing within any number
                    of threads. </para>
                <para>When the last instance of the inner-ActiveComponent scheduler object is
                    destroyed, the scheduler thread is stopped. When the last instance of a
                    scheduler proxy is destroyed, the scheduler thread is joined. It's as simple as
                    that. This makes threads shared objects.</para>
            </sect1>
            <sect1>
                <title>Object lifetime</title>
                <para>There are subtle bugs when living in a multithreaded world. Consider the
                    following class:</para>
                <para>
                    <programlisting>struct Unsafe
{
void foo()
{
  m_mutex.lock();
  // call private member
  m_mutex.unlock();
}
private:
void foobar()
{
  //we are already locked when called, do something while locked
}
boost::mutex m_mutex;
};            </programlisting>
                </para>
                <para>This is called a thread-safe interface pattern. Public members lock, private
                    do not. Simple enough, right? Unfortunately, it doesn't fly.</para>
                <para>First you have the risk of deadlock if a private member calls a public one
                    while being called from another public member. Forget to check one path of
                    execution within your class implementation and you get a nice deadlock. You'll
                    have to test every single path of execution to prove your code is correct. And
                    this at every change. Hmmm, does not sound reassuring.</para>
                <para>Anyway, let's face it, for any complex class, where there's a mutex, there is
                    a race or a deadlock...</para>
                <para>But even worse, the principle itself is not correct in C++. It supposes that a
                    class can protect itself. Well, no, it can't. Why? You can't protect the
                    destructor. If the object (and the mutex) gets destroyed when a thread waits for
                    it in foo(), we're toast. Ok, then we can use a shared_ptr, and all is good,
                    right? Then you have no destructor called as someone keeps the object alive,
                    right? Well, you still have a risk of a signal, callback, etc. </para>
                <para>What you need is protect your object with a shared_ptr and have no other way
                    to access the object. Asynchronous provides this.</para>
                <para>There are more lifetime issues, even without mutexes. If you have ever used
                    Boost.Asio, a common mistake and an easy one is when a callback is called in the
                    proactor thread after an asynchronous operation, but the object called is long
                    gone and the callback invalid. Asynchronous provides a
                        <code>trackable_servant</code> (TODO link) which makes sure that a callback
                    is not called if the object which called the asynchronous operation is gone. It
                    also prevents a task posted in a threadpool to be called if this condition
                    occurs, which improves performance. Some helper members even make sure you get
                    no crash, even while facing a Boost.Asio callback.</para>
            </sect1>
            <sect1>
                <title>Interrupting</title>
                <subtitle>Or how to catch back if you're drowning. </subtitle>
                <para>Let's say you posted so many tasks to your threadpool that all your cores are
                    full, still, your application is slipping more and more behind plan. You need to
                    give up some tasks to catch back a little.</para>
                <para>Asynchronous can give you an interruptible cookie when you post a task to a
                    scheduler, and you can use it to stop a posted task. If not running yet, the
                    task will not start, if running, it will stop at the next interruption point,
                    which are documented in the <link
                        xlink:href="http://www.boost.org/doc/libs/1_54_0/doc/html/thread/thread_management.html#thread.thread_management.tutorial.interruption"
                        >Boost.Thread documentation</link>. Diagnostics will show that a task was
                    interrupted.</para>
            </sect1>
            <sect1>
                <title>Diagnostics</title>
                <para>Finding out how good your software is doing is not an easy task. You need to
                    add lots of logging to find out which function call takes too long and becomes a
                    bottleneck. Finding out the minimum required hardware to run your application is
                    even harder.</para>
                <para>Asynchronous design helps here too. By logging the required time and the
                    frequency of tasks, it is easy to find out how many cores are needed.
                    Bottlenecks can be found by logging what the Active Component is doing and how
                    long. Finally, designing the asynchronous Active Component as state machines and
                    logging state changes will allow a better understanding of your system and make
                    visible potential for concurrency. Even for non-parallel algorithms, finding
                    out, using a state machine, the earliest point a task can be thrown to a
                    threadpool will give some easy concurrency. Throw enough tasks to the threadpool
                    and manage this with a state machine and you might use your cores with little
                    effort. Parallelization can then be improved by logging which tasks are long
                    enough to be parallelized.</para>
            </sect1>
            <sect1>
                <title>Continuations</title>
                <para>Callback are great when you have a complex flow of operations which require a
                    state machine for management, however there are cases where callbacks are not an
                    ideal solution. Either because your application requires a constant switching of
                    context between single-threaded and parallel parts, or because the
                    single-threaded part might be busy, which would delay completion of the
                    algorithm. A known example of this is a parallel fibonacci. In this case, one
                    can register a continuation, which is to be executed upon completion of one or
                    several tasks.</para>
            </sect1>
            <sect1>
                <title>Want more power? What about extra machines?</title>
                <para>What to do if your threadpools are using all of your cores but there simply
                    are not enough cores for the job? Buy more cores? Unfortunately, the number of
                    cores a single-machine can use is limited, unless you have unlimited money. A
                    dual 6-core Xeon, 24 threads with hyperthreading will cost much more than 2 x
                    6-core i7, and will usually have a lesser clock frequency and an older
                    architecture. </para>
                <para>The solution could be: start with the i7, then if you need more power, add
                    some more machines which will steal jobs from your threadpools through TCP. This
                    can be done quite easily with Asynchronous.</para>
            </sect1>
            <sect1>
                <title>Parallel algorithms</title>
                <para>The library also comes with non-blocking algorithms which fit well in the
                    asynchronous system, with more to come. If you want to contribute some more, be
                    welcome. At the moment, the library offers:<itemizedlist>
                        <listitem>
                            <para>parallel_for: with iterators or ranges, support of TCP</para>
                        </listitem>
                        <listitem>
                            <para>parallel_reduce: with iterators or ranges, support of TCP</para>
                        </listitem>
                    </itemizedlist></para>
            </sect1>
            <sect1>
                <title>Design Diagrams</title>
                <para><inlinemediaobject>
                        <imageobject>
                            <imagedata fileref="pics/AsynchronousDesign.jpg"/>
                        </imageobject>
                    </inlinemediaobject></para>
                <para>This diagram shows an overview of the design behind Asynchronous. One or more
                    Servant objects live in a single-theaded world, communicating with the outside
                    world only with one or several queues, from which the single-threaded scheduler
                    pops tasks. Tasks are pushed by calling a member on a proxy object.</para>
                <para>Like an Active Object, a client uses a proxy (a shared object type), which
                    offers the same members as the real servant, with the same parameters, the only
                    difference being the return type, a boost::future&lt;R>, with R being the return
                    type of the servant's member. All calls to a servant from the client side are
                    posted, which includes the servant constructor and destructor. When the last
                    instance of a servant is destroyed, be it used inside the Active Component or
                    outside, the servant destructor is posted.</para>
                <para>any_shared_scheduler is the part of the Active Object scheduler living inside
                    the Active Component. Servants do not hold it directly but hold an
                    any_weak_scheduler instead. The library will use it to create a posted callback
                    when a task executing in a worker threadpool is completed.</para>
                <para>Shutting down an Active Component is done automatically by not needing it. It
                    happens in the following order:<itemizedlist>
                        <listitem>
                            <para>While a servant proxy is alive, no shutdown</para>
                        </listitem>
                        <listitem>
                            <para>When the last servant proxy goes out of scope, the servant
                                destructor is posted.</para>
                        </listitem>
                        <listitem>
                            <para>if jobs from servants are running in a threadpool, they get a
                                chance to stop earlier by running into an interruption point or will
                                not even start.</para>
                        </listitem>
                        <listitem>
                            <para>threadpool(s) is (are) shut down.</para>
                        </listitem>
                        <listitem>
                            <para>The Active Component scheduler is stopped and its thread
                                terminates.</para>
                        </listitem>
                        <listitem>
                            <para>The last instance of any_shared_scheduler_proxy goes out of scope
                                with the last servant proxy and joins.</para>
                        </listitem>
                    </itemizedlist>
                </para>
                <para>It is usually accepted that threads are orthogonal to an OO design and
                    therefore are to manage as they don't belong to an object. Asynchronous comes
                    close to this: threads are not directly used, but instead owned by a scheduler,
                    in which one creates objects and tasks.</para>
            </sect1>
        </chapter>
    </part>
    <part>
        <title>User Guide</title>
        <chapter>
            <title>Using Asynchronous</title>
            <sect1>
                <title>Hello, asynchronous world</title>
                <para>The following code shows a very basic usage (a complete example <link
                        xlink:href="examples/example_post_future.cpp">here</link>), this is not really
                    asynchronous yet:</para>
                <programlisting>#include &lt;boost/asynchronous/scheduler/threadpool_scheduler.hpp>
#include &lt;boost/asynchronous/queue/lockfree_queue.hpp>
#include &lt;boost/asynchronous/scheduler_shared_proxy.hpp>
#include &lt;boost/asynchronous/post.hpp>
struct void_task
{
    void operator()()const
    {
        std::cout &lt;&lt; "void_task called" &lt;&lt; std::endl;
    }
};
struct int_task
{
    int operator()()const
    {
        std::cout &lt;&lt; "int_task called" &lt;&lt; std::endl;
        return 42;
    }
};  

// create a threadpool scheduler with 3 threads and communicate with it using a threadsafe_list
// we use auto as it is easier than boost::asynchronous::any_shared_scheduler_proxy&lt;>
auto scheduler = boost::asynchronous::create_shared_scheduler_proxy(
                            new boost::asynchronous::threadpool_scheduler&lt;
                                boost::asynchronous::lockfree_queue&lt;> >(3));
// post a simple task and wait for execution to complete
boost::shared_future&lt;void> fuv = boost::asynchronous::post_future(scheduler, void_task());
fuv.get();
// post a simple task and wait for result
boost::shared_future&lt;int> fui = boost::asynchronous::post_future(scheduler, int_task());
int res = fui.get();
  </programlisting>
                <para>Of course this works with C++11 lambdas:</para>
                <programlisting>auto scheduler = boost::asynchronous::create_shared_scheduler_proxy(
                            new boost::asynchronous::threadpool_scheduler&lt;
                                boost::asynchronous::lockfree_queue&lt;> >(3));
// post a simple task and wait for execution to complete
boost::shared_future&lt;void> fuv =
                boost::asynchronous::post_future(scheduler, [](){std::cout &lt;&lt; "void lambda" &lt;&lt; std::endl;});
fuv.get();
// post a simple task and wait for result
boost::shared_future&lt;int> fui =
                boost::asynchronous::post_future(scheduler, [](){std::cout &lt;&lt; "int lambda" &lt;&lt; std::endl;return 42;});
int res = fui.get();   </programlisting>
                <para>boost::asynchronous::post_future posts a piece of work to a threadpool
                    scheduler with 3 threads and using a simple threadsafe_list. We get a
                    boost::future&lt;the type of the task return type>.</para>
                <para>This looks like much std::async, but we're just getting started. Let's move on
                    to something more asynchronous.</para>
            </sect1>
            <sect1>
                <title>A servant proxy</title>
                <para>We now want to create a single-threaded scheduler, populate it with some
                    servant(s), and exercise some members of the servant from an outside thread. We
                    first need a servant:</para>
                <programlisting>struct Servant
{
    // optional: the servant has such an easy constructor, no need to post it
    typedef int simple_ctor;
    Servant(int data): m_data(data){}
    int doIt()const
    {
        std::cout &lt;&lt; "Servant::doIt with m_data:" &lt;&lt; m_data &lt;&lt; std::endl;
        return 5;
    }
    void foo(int&amp; i)const
    {
        std::cout &lt;&lt; "Servant::foo with int:" &lt;&lt; i &lt;&lt; std::endl;
        i = 100;
    }
    void foobar(int i, char c)const
    {
        std::cout &lt;&lt; "Servant::foobar with int:" &lt;&lt; i &lt;&lt; " and char:" &lt;&lt; c &lt;&lt;std::endl;
    }
    int m_data;
}; </programlisting>
                <para>We now create a proxy type to be used in other threads:</para>
                <programlisting>class ServantProxy : public boost::asynchronous::servant_proxy&lt;ServantProxy,<emphasis role="bold">Servant</emphasis>>
{
public:
    // forwarding constructor. Scheduler to servant_proxy, followed by arguments to Servant.
    template &lt;class Scheduler>
    ServantProxy(Scheduler s, int data):
        boost::asynchronous::servant_proxy&lt;ServantProxy,<emphasis role="bold">Servant</emphasis>>(s, data)
    {}
    // the following members must be available "outside"
    // foo and foobar, just as a post (no interesting return value)
    BOOST_ASYNC_POST_MEMBER(<emphasis role="bold">foo</emphasis>)
    BOOST_ASYNC_POST_MEMBER(<emphasis role="bold">foobar</emphasis>)
    // for doIt, we'd like a future
    BOOST_ASYNC_FUTURE_MEMBER(<emphasis role="bold">doIt</emphasis>)
};</programlisting>
                <para>Let's use our newly defined proxy:</para>
                <programlisting>int something = 3;
{
    // with c++11
    auto scheduler = boost::asynchronous::create_shared_scheduler_proxy(
                    new boost::asynchronous::single_thread_scheduler&lt;
                          boost::asynchronous::lockfree_queue&lt;> >);

    {
        // arguments (here 42) are forwarded to Servant's constructor
        ServantProxy proxy(scheduler,42);
        // post a call to foobar, arguments are forwarded.
        proxy.foobar(1,'a');
        // post a call to foo. To avoid races, the reference is ignored.
        proxy.foo(something);
        // post and get a future because we're interested in the result.
        boost::shared_future&lt;int> fu = proxy.doIt();
        std::cout&lt;&lt; "future:" &lt;&lt; fu.get() &lt;&lt; std::endl;
    }// here, Servant's destructor is posted
}// scheduler is gone, its thread has been joined
std::cout&lt;&lt; "something:" &lt;&lt; something &lt;&lt; std::endl; // something was not changed</programlisting>
                <para>We can call members on the proxy, almost as if they were called on Servant.
                    The library takes care of the posting and forwarding the arguments. When
                    required, a future is returned. Stack unwinding works, and when the servant
                    proxy goes out of scope, the servant destructor is posted. When the scheduler
                    goes out of scope, its thread is stopped and joined. The queue is processed
                    completely first. Of course, as many servants as desired can be created in this
                    scheduler context. Please have a look at <link
                        xlink:href="examples/example_simple_servant.cpp">the complete
                    example</link>.</para>
            </sect1>
            <sect1>
                <title>Using a threadpool</title>
                <para>If you remember the principles of Asynchronous, blocking a single-thread
                    scheduler is taboo as it blocks the thread doing all the management of a system.
                    But what to do when one needs to execute long tasks? Asynchronous provides a
                    whole set of threadpools. A servant posts something to a threadpool, provides a
                    callback, then gets a result. Wait a minute. Callback? Is this not
                    thread-unsafe? Why not threadpools with futures, like usual? Because in a
                    perfectly asynchronous world, waiting for a future means blocking a servant
                    scheduler. One would argue that it is possible not to block on the future, and
                    instead ask if there is a result. But then, what if not? Is the alternative to
                    poll? Like in the "good" all times?</para>
                <para>If we accept the future argument, but what about thread-safety? Asynchronous
                    takes care of this. A callback is never called from a threadpool, but instead
                    posted back to the queue of the scheduler which posted the work. All the servant
                    has to do is to do nothing and wait until the callback is executed. Note that
                    this is not the same as a blocking wait, the servant can still react to
                    events.</para>
                <para>Clearly, this brings some new challenges as the flow of control gets harder to
                    follow. This is why a servant is often written using state machines. The
                    (biased) author suggests to have a look at the <link
                        xlink:href="http://svn.boost.org/svn/boost/trunk/libs/msm/doc/HTML/index.html"
                        > Meta State Machine library </link> , which plays nicely with
                    Asynchronous.</para>
                <para>But what about the usual proactor issues of crashing when the servant has long
                    been destroyed when the callback is posted. Gone. Asynchronous provides
                        <code>trackable_servant</code> which will ensure that a callback is not
                    called if the servant is gone. Better even, if the servant has been destroyed,
                    an unstarted posted task will not be executed.</para>
                <para>Again comes another issue. And what if I post a task, say a lambda, which
                    captures a shared_ptr to an object per value, and this object is a
                    boost::signal? Then when the task object has been executed and is destroyed,
                    I'll get a race on the signal deregistration. Good point, but again no.
                    Asynchronous ensures that a task created within a scheduler context gets
                    destroyed in this context.</para>
                <para>This is about the best protection you can get. What Asynchronous cannot
                    protect you from are self-made races within a task (if you post a task with a
                    pointer to the servant, you're on your own and have to protect your servant). A
                    good rule of thumb is to consider data passed to a task as moved. To support
                    this, Asynchronous does not copy tasks but move them.</para>
                <para>Armed with these protections, let's give a try to a threadpool, starting with
                    the most basic one, <code>threadpool_scheduler</code> (more to come):</para>
                <programlisting>struct Servant : boost::asynchronous::trackable_servant&lt;>
{
    Servant(boost::asynchronous::any_weak_scheduler&lt;> scheduler)
        : boost::asynchronous::trackable_servant&lt;>(scheduler,
                                               // <emphasis role="bold">threadpool with 3 threads</emphasis> and a lockfree_queue
                                               boost::asynchronous::create_shared_scheduler_proxy(
                                                   new <emphasis role="bold">boost::asynchronous::<emphasis role="bold">threadpool_scheduler</emphasis></emphasis>&lt;
                                                           boost::asynchronous::lockfree_queue&lt;> >(<emphasis role="bold">3</emphasis>))){}
    // call to this is posted and executes in our (safe) single-thread scheduler
    void start_async_work()
    {
       //ok, let's post some work and wait for an answer
       <emphasis role="bold">post_callback</emphasis>(
                    [](){std::cout &lt;&lt; "Long Work" &lt;&lt; std::endl;}, //work, do not use "this" here
                    [/*this*/](boost::future&lt;void>){...}// callback. Safe to use "this" as callback is only called if Servant is alive
        );
    }
};</programlisting>
                <para>We now have a servant, ready to be created in its own thread, which posts some
                    long work to a 3 thread-threadpool and gets a callback, but only if still alive.
                    Similarly, the long work will be executed by the threadpool only if Servant is
                    alive. Everything else stays the same, one creates a proxy for the servant and
                    posts calls to its members, so we'll skip it for conciseness, the complete
                    example can be found <link
                        xlink:href="examples/example_post_trackable_threadpool.cpp"
                    >here</link>.</para>
            </sect1>
            <sect1>
                <title>A servant using another servant proxy</title>
                <para>Often, in a layered design, you'll need that a servant in a single-threaded
                    scheduler calls a member of a servant living in another one. And you'll want to
                    get a callback, not a future like in our previous example, because you
                    absolutely refuse to block waiting for a future (and you'll be very right of
                    course!). Ideally, except for main(), you won't want any of your objects to wait
                    for a future. There is another servant_proxy macro for this,
                        <code>BOOST_ASYNC_UNSAFE_MEMBER</code>(unsafe because you get no
                    thread-safety from if and you'll take care of this yourself, or better,
                        <code>trackable_servant</code> will take care of it for you):</para>
                <para>
                    <programlisting>// Proxy for a basic servant 
class ServantProxy : public boost::asynchronous::servant_proxy&lt;ServantProxy,Servant>
{
public:
    template &lt;class Scheduler>
    ServantProxy(Scheduler s, int data):
        boost::asynchronous::servant_proxy&lt;ServantProxy,Servant>(s, data)
    {}
    BOOST_ASYNC_UNSAFE_MEMBER(foo)
    BOOST_ASYNC_UNSAFE_MEMBER(foobar)
};   </programlisting>
                    <programlisting>// Servant using the first one
struct Servant2 : boost::asynchronous::trackable_servant&lt;>
{
    Servant2(boost::asynchronous::any_weak_scheduler&lt;> scheduler,ServantProxy worker)
        :boost::asynchronous::trackable_servant&lt;>(scheduler)
        ,m_worker(worker) // the proxy allowing access to Servant
    boost::shared_future&lt;void> doIt    
    {                 
         call_callback(m_worker.get_proxy(), // Servant's outer proxy, for posting tasks
                       m_worker.foo(), // what we want to call on Servant
                      // callback functor, when done.
                      [](boost::future&lt;int> result){...} );// future&lt;return type of foo> 
    }
};</programlisting>
                </para>
                <para>Call of <code>foo()</code> will be posted to <code>Servant</code>'s scheduler,
                    and the callback lambda will be posted to <code>Servant2</code> when completed.
                    All this thread-safe of course. Destruction is also safe. When
                        <code>Servant2</code> goes out of scope, it will shutdown
                        <code>Servant</code>'s scheduler, then will his scheduler be shutdown
                    (provided no more object is living there), and all threads joined. The <link
                        xlink:href="examples/example_two_simple_servants.cpp">complete example
                    </link> shows a few more calls too.</para>
            </sect1>
            <sect1>
                <title>Interrupting tasks</title>
                <para>Imagine a manager object (a state machine for example) posted some
                    long-lasting work to a threadpool, but this long-lasting work really takes too
                    long. As we are in an asynchronous world and non-blocking, the manager object
                    realizes there is a problem and decides the task must be stopped otherwise the
                    whole application starts failing some real-time constraints. This is also
                    possible, one uses another version of posting, gets some handle, on which one
                    can require interruption. As Asynchronous does not kill threads, it means that
                    we'll have to use one of Boost.Thread predefined interruption points. Supposing
                    we have well-behaved tasks, they will be interrupted at the next interruption
                    point if they started, or if they did not start yet because they are waiting in
                    a queue, then they will never start. In this <link
                        xlink:href="examples/example_interrupt.cpp">example</link>, we have very
                    little to change but the post call:</para>
                <programlisting>struct Servant : boost::asynchronous::trackable_servant&lt;>
{
     ... // as usual
    void start_async_work()
    {
        // start long interruptible tasks
        // we get an interruptible handler representing the task
        <emphasis role="bold">boost::asynchronous::any_interruptible</emphasis> interruptible =
        <emphasis role="bold">interruptible_post_callback</emphasis>(
                // interruptible task
               [](){
                    std::cout &lt;&lt; "Long Work" &lt;&lt; std::endl;
                    boost::this_thread::sleep(boost::posix_time::milliseconds(1000));}, // sleep is an interrupting point
               // callback functor.
               [](boost::future&lt;void> ){std::cout &lt;&lt; "Callback will most likely not be called" &lt;&lt; std::endl;}
        );
        // let the task start (not sure but likely)
        // if it had no time to start, well, then it will never.
        boost::this_thread::sleep(boost::posix_time::milliseconds(100));
        // actually, we changed our mind and want to interrupt the task
        interruptible.interrupt();
        // the callback will never be called as the task was interrupted
    }
};                </programlisting>
            </sect1>
            <sect1>
                <title>Logging tasks</title>
                <para>Developers are notoriously famous for being bad at guessing which part of
                    their code is inefficient or has potential for long execution time. This is bad
                    in itself, but even worse for a control class like our post-callback servant as
                    it reduces responsiveness. Knowing how long a posted call or a callback lasts is
                    therefore very useful. Knowing how long take tasks executing in the threadpools
                    is also essential to plan what hardware one needs for an application(4 cores? Or
                    100?). We need to know what our program is doing. Asynchronous provides some
                    logging per task to help there. Let's have a look at some code. It's also time
                    to start using our template parameters for <code>trackable_servant</code>, in case you
                    wondered why they are here.</para>
                <programlisting>// we will be using loggable jobs internally
typedef boost::asynchronous::any_loggable&lt;boost::chrono::high_resolution_clock> <emphasis role="bold">servant_job</emphasis>;
// the type of our log
typedef std::map&lt;std::string,std::list&lt;boost::asynchronous::diagnostic_item&lt;boost::chrono::high_resolution_clock> > > <emphasis role="bold">diag_type</emphasis>;
// we log our scheduler and our threadpool scheduler (both use servant_job)
struct Servant : boost::asynchronous::trackable_servant&lt;<emphasis role="bold">servant_job</emphasis>,<emphasis role="bold">servant_job</emphasis>>
{
    Servant(boost::asynchronous::any_weak_scheduler&lt;servant_job> scheduler) //servant_job is our job type
        : boost::asynchronous::trackable_servant&lt;<emphasis role="bold">servant_job,servant_job</emphasis>>(scheduler,
                                               boost::asynchronous::create_shared_scheduler_proxy(
                                                   // threadpool with 3 threads and a simple threadsafe_list queue
                                                   // Furthermore, it logs posted tasks
                                                   new boost::asynchronous::threadpool_scheduler&lt;
                                                           //servant_job is our job type
                                                           boost::asynchronous::lockfree_queue&lt; <emphasis role="bold">servant_job</emphasis> > >(3))){}
    void start_async_work()
    {
         post_callback(
               // task posted to threadpool
               [](){...}, // will return an int
               [](boost::future&lt;int> res){...},// callback functor.
               // the task / callback name for logging
               <emphasis role="bold">"int_async_work"</emphasis>
        );
    }
    // we happily provide a way for the outside world to know what our threadpool did.
    // get_worker is provided by trackable_servant and gives the proxy of our threadpool
    diag_type get_diagnostics() const
    {
        return (*get_worker()).get_diagnostics();
    }
};</programlisting>
                <para>The proxy is also slightly different, as it uses a _LOG macro and an argument
                    representing the name of the task.</para>
                <programlisting>class ServantProxy : public boost::asynchronous::servant_proxy&lt;ServantProxy,Servant,servant_job>
{
public:
    template &lt;class Scheduler>
    ServantProxy(Scheduler s):
        boost::asynchronous::servant_proxy&lt;ServantProxy,Servant,servant_job>(s)
    {}
    // the _LOG macros do the same as the others, but take an extra argument, the logged task name
    BOOST_ASYNC_FUTURE_MEMBER<emphasis role="bold">_LOG</emphasis>(start_async_work,<emphasis role="bold">"proxy::start_async_work"</emphasis>)
    BOOST_ASYNC_FUTURE_MEMBER<emphasis role="bold">_LOG</emphasis>(get_diagnostics,<emphasis role="bold">"proxy::get_diagnostics"</emphasis>)
    };               </programlisting>
                <para> We now can get diagnostics from both schedulers, the single-threaded and the
                    threadpool (as external code has no access to it, we ask Servant to help us
                    there through a get_diagnostics() member).</para>
                <programlisting>// create a scheduler with logging
auto scheduler = boost::asynchronous::create_shared_scheduler_proxy(
                                new boost::asynchronous::single_thread_scheduler&lt;
                                    boost::asynchronous::lockfree_queue&lt;servant_job> >);
// create a Servant                    
ServantProxy proxy(scheduler); 
...
// let's ask the single-threaded scheduler what it did.
diag_type single_thread_sched_diag = (*scheduler).get_diagnostics(); 
for (auto mit = single_thread_sched_diag.begin(); mit != single_thread_sched_diag.end() ; ++mit)
{
     std::cout &lt;&lt; "job type: " &lt;&lt; (*mit).first &lt;&lt; std::endl;
     for (auto jit = (*mit).second.begin(); jit != (*mit).second.end();++jit)
     {
          std::cout &lt;&lt; "job waited in us: " &lt;&lt; boost::chrono::nanoseconds((*jit).get_started_time() - (*jit).<emphasis role="bold">get_posted_time()</emphasis>).count() / 1000 &lt;&lt; std::endl;
          std::cout &lt;&lt; "job lasted in us: " &lt;&lt; boost::chrono::nanoseconds((*jit).get_finished_time() - (*jit).<emphasis role="bold">get_started_time()</emphasis>).count() / 1000 &lt;&lt; std::endl;
          std::cout &lt;&lt; "job interrupted? "  &lt;&lt; std::boolalpha &lt;&lt; (*jit).<emphasis role="bold">is_interrupted()</emphasis> &lt;&lt; std::endl;
     }
     }              </programlisting>
                <para>It goes similarly with the threapool scheduler, with the slight difference
                    that we ask the Servant to deliver diagnostic information through a proxy
                    member. The <link xlink:href="examples/example_log.cpp">complete example</link>
                    shows all this, plus an interrupted job (you might have noticed in the previous
                    listing that a diagnostic offers an <code>is_interrupted</code> member).</para>
            </sect1>
            <sect1>
                <title>Queue container with priority</title>
                <para>Sometimes, all jobs posted to a scheduler do not have the same priority. For
                    threadpool schedulers, <code>composite_threadpool_scheduler</code> is an option.
                    For a single-threaded scheduler, Asynchronous does not provide a priority queue
                    but a queue container, which itself contains any number of queues, of different
                    types if needed. This has several advantages:<itemizedlist>
                        <listitem>
                            <para>Priority is defined simply by posting to the queue with the
                                desired priority, so there is no need for expensive priority
                                algorithms.</para>
                        </listitem>
                        <listitem>
                            <para>Ones gets also reduced contention if many threads of a threadpool
                                post something to the queue of a single-threaded scheduler. If no
                                priority is defined, one queue will be picked, according to a
                                configurable policy, reducing contention on a single queue.</para>
                        </listitem>
                        <listitem>
                            <para>It is possible to mix queues to get the best of each.</para>
                        </listitem>
                        <listitem>
                            <para>One can build a queue container of queue containers, etc.</para>
                        </listitem>
                    </itemizedlist></para>
                <para>Note: This applies to any scheduler. We'll start with single-threaded
                    schedulers used by managing servants for simplicity, but it is possible to have
                    composite schedulers using queue containers for finest granularity and least
                    contention.</para>
                <para>First, we need to create a single-threaded scheduler with several queues for
                    our servant to live in, for example, one threadsafe list and and lockfree
                    queues:</para>
                <programlisting>boost::asynchronous::any_shared_scheduler_proxy&lt;> scheduler = 
                           boost::asynchronous::create_shared_scheduler_proxy(
                                new boost::asynchronous::single_thread_scheduler&lt;
                                        boost::asynchronous::any_queue_container&lt;> >
                        (boost::asynchronous::any_queue_container_config&lt;boost::asynchronous::threadsafe_list&lt;> >(1),
                         boost::asynchronous::any_queue_container_config&lt;boost::asynchronous::lockfree_queue&lt;> >(3,100)
                         ));</programlisting>
                <para><code>any_queue_container</code> takes as constructor arguments a variadic
                    sequence of <code>any_queue_container_config</code>, with a queue type as
                    template argument, and in the constructor the number of objects of this queue
                    (in the above example, one <code>threadsafe_list</code> and 3
                        <code>lockfree_queue</code> instances, then the parameters that these queues
                    require in their constructor (100 is the capacity of the underlying
                        <code>boost::lockfree_queue</code>). This means, that our
                        <code>single_thread_scheduler</code> has 4 queues:<itemizedlist>
                        <listitem>
                            <para>a threadsafe_list at index 1</para>
                        </listitem>
                        <listitem>
                            <para>lockfree queues at indexes 2,3,4</para>
                        </listitem>
                        <listitem>
                            <para>>= 4 means the queue with the least priority.</para>
                        </listitem>
                        <listitem>
                            <para>0 means "any queue" and is the default</para>
                        </listitem>
                    </itemizedlist></para>
                <para>The scheduler will handle these queues as having priorities: as long as there
                    are work items in the first queue, take them, if there are no, try in the
                    second, etc. If all queues are empty, the thread gives up his time slice and
                    sleeps until some work item arrives. If no priority is defined by posting, a
                    queue will be chosen (by default randomly, but you can configure this with a
                    policy). This has the advantage of reducing contention of the queue, even when
                    not using priorities. The servant defines the priority of the tasks it provides.
                    While this might seem surprising, it is a design choice to avoid someone using a
                    servant proxy interface to think about it, as you will see in the second
                    listing. To define a priority for a servant proxy, there is a second field in
                    the macros:</para>
                <programlisting>class ServantProxy : public boost::asynchronous::servant_proxy&lt;ServantProxy,Servant>
{
public:
    template &lt;class Scheduler>
    ServantProxy(Scheduler s):
        boost::asynchronous::servant_proxy&lt;ServantProxy,Servant>(s)
    {}
    <emphasis role="bold">BOOST_ASYNC_SERVANT_POST_CTOR(3)</emphasis>
    <emphasis role="bold">BOOST_ASYNC_SERVANT_POST_DTOR(4)</emphasis>
    BOOST_ASYNC_FUTURE_MEMBER(start_async_work,<emphasis role="bold">1</emphasis>)
};</programlisting>
                <para>BOOST_ASYNC_FUTURE_MEMBER and other similar macros can be given an optional
                    priority parameter, in this case 1, which is our threadsafe list. Notice how you
                    can then define the priority of the posted servant constructor and
                    destructor.</para>
                <programlisting>ServantProxy proxy(scheduler);
                boost::shared_future&lt;boost::shared_future&lt;int> > fu = proxy.start_async_work();</programlisting>
                <para>Calling our proxy member stays unchanged because the macro defines the
                    priority of the call.</para>
                <para>We also have an extended version of <code>post_callback</code>, called by a
                    servant posting work to a threadpool:</para>
                <programlisting>post_callback(
               [](){return 42;}// work
                ,
               [this](boost::shared_future&lt;int> res){}// callback functor.
               ,"",
               <emphasis role="bold">2,2</emphasis>
               );</programlisting>
                <para>Note the two added priority values: the first one for the task posted to the
                    threadpool, the second for the priority of the callback posted back to the
                    servant scheduler. The string is the log name of the task, which we choose to
                    ignore here.</para>
                <para>The priority is in any case an indication, the scheduler is free to ignore it
                    if not supported. In the <link xlink:href="examples/example_queue_container.cpp"
                        >example</link>, the single threaded scheduler will honor the request, but
                    the threadpool has a normal queue and cannot honor the request, but a threadpool
                    with an <code>any_queue_container</code> or a
                        <code>composite_threadpool_scheduler</code> can. The <link
                        xlink:href="examples/example_queue_container_log.cpp">same example</link>
                    can be rewritten to make use of the logging mechanism.</para>
                <para><code>any_queue_container</code> has two template arguments. The first, the
                    job type, is as always, a callable (<code>any_callable</code>) job. The second
                    is the policy which Asynchronous usses to find the desired queue for a job. The
                    default is <code>default_find_position</code>, which is as described above, 0
                    means any position, all other values map to a queue, priorities >= number of
                    queues means last queue. Any position is by default random
                        (<code>default_random_push_policy</code>), but you might pick
                        <code>sequential_push_policy</code>, which keeps an atomic counter and
                    always adds to the next queue.</para>
                <para>If your idea is to build a queue container of queue containers, you'll
                    probably want to provide your own policy.</para>
            </sect1>
            <sect1>
                <title>Multiqueue Schedulers' priority</title>
                <para>TODO</para>
            </sect1>
            <sect1>
                <title>Threadpool Schedulers with several queues</title>
                <para>A queue container has advantages (different queue types, priority for single
                    threaded schedulers) but also disadvantages (takes jobs from one end of the
                    queue, which means potential cache misses, more typing work). If you don't need
                    different queue types for a threadpool but want to reduce contention, multiqueue
                    schedulers are for you. A normal <code>threadpool_scheduler</code> has x threads
                    and one queue, serving them. A <code>multiqueue_threadpool_scheduler</code> has
                    x threads and x queues, each serving a worker thread. Each thread looks for work
                    in its queue. If it doesn't find any, it looks for work in the previous one,
                    etc. until it finds one or inspected all the queues. As all threads steal from
                    the previous queue, there is little contention. The construction of this
                    threadpool is very similar to the simple
                    <code>threadpool_scheduler</code>:</para>
                <programlisting>boost::asynchronous::any_shared_scheduler_proxy&lt;> scheduler = 
    boost::asynchronous::create_shared_scheduler_proxy(
                // 4 threads and 4 lockfree queues of 10 capacity
                new boost::asynchronous::multiqueue_threadpool_scheduler&lt;boost::asynchronous::lockfree_queue&lt;> >(4,10));</programlisting>
                <para>The first argument is, as usual, the number of worker threads, which is at the
                    same time the number of queues. As for every scheduler, if the queue constructor
                    takes arguments, they come next and are forwarded to the queue.</para>
                <para>This is the advised scheduler for standard cases as it offers lesser
                    contention and task stealing between the queues it uses for task
                    transfer.</para>
                <para>There is a limitation, these schedulers cannot have 0 thread like their
                    single-queue counterparts.</para>
            </sect1>
            <sect1>
                <title>composite_threadpool_scheduler</title>
                <para>When a project becomes more complex, having a single threadpool for the whole
                    application does not offer enough flexibility in load planning. It is pretty
                    hard to avoid either oversubscription (more busy threads than available hardware
                    threads) or undersubscription. One would need one big threadpool with exactly
                    the number of threads available in the hardware. Unfortunately, if we have a
                    hardware with, say 12 hardware threads, parallelizing some work using all 12
                    might be slowlier than using only 8. One would need different threadpools of
                    different number of threads for the application. This, however, has the serious
                    drawback that there is a risk that some threadpools will be in overload, while
                    others are out of work unless we have work stealing between different
                    threadpools.</para>
                <para>The second issue is task priority. One can define priorities with several
                    queues or a queue container, but this ensures that only highest priority tasks
                    get executed if the system is coming close to overload. Ideally, it would be
                    great if we could decide how much compute power we give to each task
                    type.</para>
                <para>This is what <code>composite_threadpool_scheduler</code> solves. This pool
                    supports, like any other pool, the
                    <code>any_shared_scheduler_proxy</code>concept so you can use it in place of the
                    ones we used so far. The pool is composed of other pools
                        (<code>any_shared_scheduler_proxy</code> pools). It implements work stealing
                    between pools if a) the pools support it and b) the queue of a pool also does.
                    For example, if we define as worker of a servant inside a single-threaded
                    scheduler:</para>
                <para>
                    <programlisting>// create a composite threadpool made of:
// a multiqueue_threadpool_scheduler, 1 thread, with a lockfree_queue of capacity 100. 
// This scheduler does not steal from other schedulers, but will lend its queue for stealing
boost::asynchronous::any_shared_scheduler_proxy&lt;> tp = boost::asynchronous::create_shared_scheduler_proxy( 
               new boost::asynchronous::multiqueue_threadpool_scheduler&lt;boost::asynchronous::lockfree_queue&lt;> > (1,100));

// a stealing_multiqueue_threadpool_scheduler, 3 threads, each with a threadsafe_list
// this scheduler will steal from other schedulers if it can. In this case it will manage only with tp, not tp3
boost::asynchronous::any_shared_scheduler_proxy&lt;> tp2 = boost::asynchronous::create_shared_scheduler_proxy( 
                    new boost::asynchronous::stealing_multiqueue_threadpool_scheduler&lt;boost::asynchronous::threadsafe_list&lt;> > (3));

// a multiqueue_threadpool_scheduler, 4 threads, each with a lockfree_spsc_queue of capacity 100
// this works because there will be no stealing as the queue can't, and only this single-thread scheduler will be the producer
boost::asynchronous::any_shared_scheduler_proxy&lt;> tp3 = boost::asynchronous::create_shared_scheduler_proxy( 
               new boost::asynchronous::multiqueue_threadpool_scheduler&lt;boost::asynchronous::lockfree_spsc_queue&lt;> > (4,100));

// create a composite pool made of the 3 previous ones
boost::asynchronous::any_shared_scheduler_proxy&lt;> tp_worker =
             boost::make_shared&lt;boost::asynchronous::composite_threadpool_scheduler&lt;> > (tp,tp2,tp3);
                    </programlisting>
                </para>
                <para>We can use this pool:<itemizedlist>
                        <listitem>
                            <para>As a big worker pool. In this case, the priority argument we use
                                for posting refers to the (1-based) index of the subpool
                                (post_callback(func1,func2,"task name",<emphasis role="bold"
                                    >1</emphasis>,0);). "1" means post to the first pool. But the
                                second pool could steal the work.</para>
                        </listitem>
                        <listitem>
                            <para>As a pool container, but different parts of the code will get to
                                see only the subpools. For example, the pools tp, tp2 and tp3 can
                                still be used independently as a worker pool. Calling
                                composite_threadpool_scheduler&lt;>::get_scheduler(std::size_t
                                index_of_pool) will also give us the corresponding pool (1-based, as
                                always).</para>
                        </listitem>
                    </itemizedlist></para>
                <para>A good example of why to use this pool is if you have a threadpool for an
                    asio-based communication. Using such a pool inside the composite pool will allow
                    the threads of this pool to help (steal) other pools if they have nothing to do. </para>
                <para>Stealing is done with priority. A stealing poll first tries to steal from the
                    first pool, then from the second, etc.</para>
                <para>The <link xlink:href="examples/example_composite_threadpool.cpp">following
                        example</link> shows a complete servant implementation, and the ASIO section
                    will show how an ASIO pool can steal (TODO link).</para>
                <para>The threadpool schedulers we saw so far are not stealing from other pools. The
                    single-queue schedulers are not stealing, and the multiqueue schedulers steal
                    from the queues of other threads of the same pool. The stealing schedulers
                    usually indicate this by appending a <code>stealing_</code> to their name:<itemizedlist>
                        <listitem>
                            <para><code>stealing_threadpool_scheduler</code> is a
                                    <code>threadpool_scheduler</code> which steals from other
                                pools.</para>
                        </listitem>
                        <listitem>
                            <para><code>stealing_multiqueue_threadpool_scheduler</code> is a
                                    <code>multiqueue_threadpool scheduler</code> which steals from
                                other pools.</para>
                        </listitem>
                        <listitem>
                            <para><code>asio_scheduler steals</code>.</para>
                        </listitem>
                    </itemizedlist></para>
                <para>The only difference with their not stealing equivalent is that they need a
                    composite_scheduler to tell them from which queues they can steal.</para>
                <para>Not all schedulers offer a queue to steal from. A
                        <code>single_thread_scheduler</code> does not as it would likely bring race
                    conditions in active objects. If you do want to allow stealing, use a threadpool
                    with 1 thread. An <code>asio_scheduler</code> also offers no queue to steal from
                    although it can steal from other queues because Boost.Asio does not offer this
                    in its interface. Future extensions will overcome this.</para>
                <para>TODO priority posting</para>
            </sect1>
            <sect1>
                <title>asio_scheduler</title>
                <para>Asynchronous supports the possibility to use Boost.Asio as a threadpool
                    provider. This has several advantages:<itemizedlist>
                        <listitem>
                            <para>asio_scheduler is delivered with a way to access Asio's io_service
                                from a servant object living inside the scheduler.</para>
                        </listitem>
                        <listitem>
                            <para>asio_scheduler handles the necessary work for creating a pool of
                                threads for multithreaded-multi-io_service communication.</para>
                        </listitem>
                        <listitem>
                            <para>asio_scheduler threads implement work-stealing from other
                                Asynchronous schedulers. This allows communication threads to help
                                other threadpools when no I/O communication is happening. This helps
                                reducing thread oversubscription.</para>
                        </listitem>
                        <listitem>
                            <para>One has all the usual goodies of Asynchronous: safe callbacks,
                                object tracking, servant proxies, etc.</para>
                        </listitem>
                    </itemizedlist></para>
                <para>Let's create a simple but powerful example to illustrate its usage. We want to
                    create a TCP client, which connects several times to the same server, gets data
                    from it (in our case, the Boost license will do), then checks if the data is
                    coherent by comparing the results two-by-two. Of course, the client has to be
                    perfectly asynchronous and never block. We also want to guarantee some threads
                    for the communication and some for the calculation work. We also want to
                    communication threads to "help" by stealing some work if necessary.</para>
                <para>Let's start by creating a TCP client using Boost.Asio. A slightly modified
                    version of the async TCP client from the Asio documentation will do. All we
                    change is pass it a callback which it will call when the requested data is
                    ready. We now pack it into an Asynchronous trackable client:</para>
                <programlisting>// Objects of this type are made to live inside an asio_scheduler,
// they get their associated io_service object from TLS
struct AsioCommunicationServant : boost::asynchronous::trackable_servant&lt;>
{
    AsioCommunicationServant(boost::asynchronous::any_weak_scheduler&lt;> scheduler,
                             const std::string&amp; server, const std::string&amp; path)
        : boost::asynchronous::trackable_servant&lt;>(scheduler)
        , m_client(*<emphasis role="bold">boost::asynchronous::get_io_service&lt;>()</emphasis>,server,path)
    {}
    void test(std::function&lt;void(std::string)> cb)
    {
        // just forward call to asio asynchronous http client
        // the only change being the (safe) callback which will be called when http get is done
        m_client.request_content(cb);
    }
private:
    client m_client; //client is from Asio example
};</programlisting>
                <para>The main noteworthy thing to notice is the call to <emphasis role="bold"
                        >boost::asynchronous::get_io_service&lt;>()</emphasis>, which, using
                    thread-local-storage, gives us the io_service associated with this thread (one
                    io_service per thread). This is needed by the Asio TCP client. Also noteworthy
                    is the argument to <code>test()</code>, a callback when the data is available. </para>
                <para>Wait a minute, is this not unsafe (called from an asio worker thread)? It is
                    but it will be made safe in a minute.</para>
                <para>We now need a proxy so that this communication servant can be safely used by
                    others, as usual:</para>
                <programlisting>class AsioCommunicationServantProxy: public boost::asynchronous::servant_proxy&lt;AsioCommunicationServantProxy,AsioCommunicationServant >
{
public:
    // ctor arguments are forwarded to AsioCommunicationServant
    template &lt;class Scheduler>
    AsioCommunicationServantProxy(Scheduler s,const std::string&amp; server, const std::string&amp; path):
        boost::asynchronous::servant_proxy&lt;AsioCommunicationServantProxy,AsioCommunicationServant >(s,server,path)
    {}
    // we offer a single member for posting
    BOOST_ASYNC_POST_MEMBER(test)
};                   </programlisting>
                <para>A single member, <code>test</code>, is used in the proxy. The constructor
                    takes the server and relative path to the desired page. We now need a manager
                    object, which will trigger the communication, wait for data, check that the data
                    is coherent:</para>
                <programlisting>struct Servant : boost::asynchronous::trackable_servant&lt;>
{
    Servant(boost::asynchronous::any_weak_scheduler&lt;> scheduler,const std::string&amp; server, const std::string&amp; path)
        : boost::asynchronous::trackable_servant&lt;>(scheduler)
        , m_check_string_count(0)
    {
        // as worker we use a simple threadpool scheduler with 4 threads (0 would also do as the asio pool steals)
        auto worker_tp = boost::asynchronous::create_shared_scheduler_proxy(
                    new boost::asynchronous::threadpool_scheduler&lt;boost::asynchronous::lockfree_queue&lt;> > (4,10));

        // for tcp communication we use an asio-based scheduler with 3 threads
        auto asio_workers = boost::asynchronous::create_shared_scheduler_proxy(new boost::asynchronous::asio_scheduler&lt;>(3));

        // we create a composite pool whose only goal is to allow asio worker threads to steal tasks from the threadpool
        m_pools = boost::asynchronous::create_shared_scheduler_proxy(
                    new boost::asynchronous::composite_threadpool_scheduler&lt;> (worker_tp,asio_workers));

        set_worker(worker_tp);
        // we create one asynchronous communication manager in each thread
        m_asio_comm.push_back(AsioCommunicationServantProxy(asio_workers,server,path));
        m_asio_comm.push_back(AsioCommunicationServantProxy(asio_workers,server,path));
        m_asio_comm.push_back(AsioCommunicationServantProxy(asio_workers,server,path));
    }
... //to be continued                 
                </programlisting>
                <para>We create 3 pools:<itemizedlist>
                        <listitem>
                            <para>A worker pool for calculations (page comparisons)</para>
                        </listitem>
                        <listitem>
                            <para>An asio threadpool with 3 threads in which we create 3
                                communication objects.</para>
                        </listitem>
                        <listitem>
                            <para>A composite pool which binds both pools together into one stealing
                                unit. You could even set the worker pool to 0 thread, in which case
                                the worker will get its work done when the asio threads have nothing
                                to do. Only non- multiqueue schedulers support this. This composite
                                pool is now made to be the worker pool of this object using
                                    <code>set_worker()</code>.</para>
                        </listitem>
                    </itemizedlist></para>
                <para>We then create our communication objects inside the asio pool.</para>
                <para><emphasis role="underline">Note</emphasis>: at the moment, asio pools can
                    steal from other pools but not be stolen from. Let's move on to the most
                    interesting part:</para>
                <programlisting>void get_data()
{
    // provide this callback (executing in our thread) to all asio servants as task result. A string will contain the page
    std::function&lt;void(std::string)> f =            
...
    m_asio_comm[0].test(make_safe_callback(f));
    m_asio_comm[1].test(make_safe_callback(f));
    m_asio_comm[2].test(make_safe_callback(f));
    }</programlisting>
                <para>We skip the body of f for the moment. f is task which will be posted to each
                    communication servant so that they can do the same work:<itemizedlist>
                        <listitem>
                            <para>call the same http get on an asio servants</para>
                        </listitem>
                        <listitem>
                            <para>at each callback, check if we got all three callbacks</para>
                        </listitem>
                        <listitem>
                            <para>if yes, post some work to worker threadpool, compare the returned
                                strings (should be all the same)</para>
                        </listitem>
                        <listitem>
                            <para>if all strings equal as they should be, cout the page</para>
                        </listitem>
                    </itemizedlist></para>
                <para>All this will be doine in a single functor. This functor is passed to each
                    communication servant, packed into a make_safe_callback, which, as its name
                    says, transforms the unsafe functor into one which posts this callback functor
                    to the manager thread and also tracks it to check if still alive at the time of
                    the callback. By calling <code>test()</code>, we trigger the 3 communications,
                    and f will be called 3 times. The body of f is:</para>
                <programlisting>std::function&lt;void(std::string)> f =
                [this](std::string s)
                {
                   this->m_requested_data.push_back(s);
                   // poor man's state machine saying we got the result of our asio requests :)
                   if (this->m_requested_data.size() == 3)
                   {
                       // ok, this has really been called for all servants, compare.
                       // but it could be long, so we will post it to threadpool
                       std::cout &lt;&lt; "got all tcp data, parallel check it's correct" &lt;&lt; std::endl;
                       std::string s1 = this->m_requested_data[0];
                       std::string s2 = this->m_requested_data[1];
                       std::string s3 = this->m_requested_data[2];
                       // this callback (executing in our thread) will be called after each comparison
                       auto cb1 = [this,s1](boost::future&lt;bool> res)
                       {
                          if (res.get())
                              ++this->m_check_string_count;
                          else
                              std::cout &lt;&lt; "uh oh, the pages do not match, data not confirmed" &lt;&lt; std::endl;
                          if (this->m_check_string_count ==2)
                          {
                              // we started 2 comparisons, so it was the last one, data confirmed
                              std::cout &lt;&lt; "data has been confirmed, here it is:" &lt;&lt; std::endl;
                              std::cout &lt;&lt; s1;
                          }
                       };
                       auto cb2=cb1;
                       // post 2 string comparison tasks, provide callback where the last step will run
                       this->post_callback([s1,s2](){return s1 == s2;},std::move(cb1));
                       this->post_callback([s2,s3](){return s2 == s3;},std::move(cb2));
                   }
                };        
                </programlisting>
                <para> We start by checking if this is the third time this functor is called (this,
                    the manager, is nicely serving as holder, kind of poor man's state machine
                    counting to 3). If yes, we prepare a call to the worker pool to compare the 3
                    returned strings 2 by 2 (cb1, cb2). Again, simple state machine, if the callback
                    is called twice, we are done comparing string 1 and 2, and 2 and 3, in which
                    case the page is confirmed and cout'ed. The last 2 lines trigger the work and
                    post to our worker pool (which is the threadpool scheduler, or, if stealing
                    happens, the asio pool) two comparison tasks and the callbacks.</para>
                <para>Our manager is now ready, we still need to create for it a proxy so that it
                    can be called from the outside world asynchronously, then create it in its own
                    thread, as usual:</para>
                <programlisting>class ServantProxy : public boost::asynchronous::servant_proxy&lt;ServantProxy,Servant>
{
public:
    template &lt;class Scheduler>
    ServantProxy(Scheduler s,const std::string&amp; server, const std::string&amp; path):
        boost::asynchronous::servant_proxy&lt;ServantProxy,Servant>(s,server,path)
    {}
    // get_data is posted, no future, no callback
    BOOST_ASYNC_POST_MEMBER(get_data)
};
...              
auto scheduler = boost::asynchronous::create_shared_scheduler_proxy(
                                new boost::asynchronous::single_thread_scheduler&lt;
                                     boost::asynchronous::threadsafe_list&lt;> >);
{
   ServantProxy proxy(scheduler,"www.boost.org","/LICENSE_1_0.txt");
   // call member, as if it was from Servant
   proxy.get_data();
   // if too short, no problem, we will simply give up the tcp requests
   // this is simply to simulate a main() doing nothing but waiting for a termination request
   boost::this_thread::sleep(boost::posix_time::milliseconds(2000));
}
                </programlisting>
                <para> As usual, <link xlink:href="examples/example_asio_http_client.cpp">here the
                        complete, ready-to-use example</link> and the implementeation of the <link
                        xlink:href="examples/asio/asio_http_async_client.cpp">Boost.Asio HTTP
                        client</link>. </para>
            </sect1>
            <sect1>
                <title>Timers</title>
                <para>Very often, an Active Object servant acting as an asynchronous dispatcher will
                    post tasks which have to be done until a certain point in the future, or which
                    will start only at a later point. State machines also regularly make use of a
                    "time" event.</para>
                <para>For this we need a timer, but a safe one:<itemizedlist>
                        <listitem>
                            <para>The timer callback has to be posted to the Active Object thread to
                                avoid races.</para>
                        </listitem>
                        <listitem>
                            <para>The timer callback shall not be called in the servant making the
                                request has been deleted (it can be an awfully long time until the
                                callback).</para>
                        </listitem>
                    </itemizedlist></para>
                <para>Asynchronous itself has no timer, but Boost.Asio has, so the library provides
                    a wrapper around it and will allow us to create a timer using an io_service
                    running in its own thread or in an asio threadpool, also provided by the
                    library.</para>
                <sect2>
                    <title>Constructing a timer</title>
                    <para>One first needs an <code>asio_scheduler</code> with at least one
                        thread:</para>
                    <programlisting>boost::asynchronous::any_shared_scheduler_proxy&lt;> asio_sched = boost::asynchronous::create_shared_scheduler_proxy(new boost::asynchronous::asio_scheduler&lt;>(1));               
                    </programlisting>
                    <para>The Servant living in its ActiveObject thread then creates a timer (as
                        attribute to keep it alive as destroying the object will cancel the timer)
                        using this scheduler and a timer value:</para>
                    <programlisting> boost::asynchronous::asio_deadline_timer_proxy m_timer (asio_sched,boost::posix_time::milliseconds(1000));                   
                    </programlisting>
                    <para>It can now start the timer using <code>trackable_servant</code> (its base
                            class)<code>::async_wait</code>, passing it a functor call when timer
                        expires / is cancelled:</para>
                    <programlisting> async_wait(m_timer,
            [](const ::boost::system::error_code&amp; err)
            {
                std::cout &lt;&lt; "timer expired? "&lt;&lt; std::boolalpha &lt;&lt; (bool)err &lt;&lt; std::endl; //true if expired, false if cancelled
            } 
            );                  </programlisting>
                    <para>Canceling the timer means destroying (and possibly recreating) the timer
                        object:</para>
                    <programlisting> m_timer =  boost::asynchronous::asio_deadline_timer_proxy(get_worker(),boost::posix_time::milliseconds(1000));                                   
                    </programlisting>
                    <para>The <link xlink:href="examples/example_asio_deadline_timer.cpp">following
                            example </link> displays a servant using an asio scheduler as a thread
                        pool and creating there its timer object. Not how the timer is created using
                            <code>trackable_servant</code> (its base
                            class)<code>::get_worker()</code>.</para>
                </sect2>
            </sect1>
            <sect1>
                <title>Continuation tasks</title>
                <para>A common limitation of threadpools is support for recursive tasks: tasks start
                    other tasks, which start other tasks, until all threads in the threadpool are
                    busy waiting. At this point, one could add more threads, but threads are
                    expensive. Similarly, you might post a task which posts more tasks and wait for
                    them to complete to do a merge of the result. Of course you can achieve this
                    with a controller object or state machine in a single-threaded scheduler waiting
                    for callbacks, but for very small tasks, using callbacks might just be too
                    expensive. In such cases, Asynchronous provides continuations: a task executes,
                    does something then creates a continuation which will wake up when ready.</para>
                <para>A common example of recursive tasks is a parallel fibonacci. Usually, this
                    means a task calculating fib(n) will start a fib(n-1) and fib(n-2) and blocks
                    until both are done. These tasks will start more tasks, etc. until a cutoff
                    number, at which point recursion stops and fibonacci is calculated serially.
                    This approach has some problems: to avoid thread explosion, we would need
                    fibers, which are not available in Boost at the time of this writing, and even
                    in fibers, tasks would block, which means interrupting them is not possible,
                    which we would want to avoid. In any case, blocking simply isn't part of the
                    asynchronous philosophy of the library. Let's have a look how continuation tasks
                    let us implement a parallel fibonacci.</para>
                <para>First of all, we need a serial fibonacci to use for the cutoff. This is a
                    classical one:</para>
                <programlisting> long serial_fib( long n ) {
    if( n&lt;2 )
        return n;
    else
        return serial_fib(n-1)+serial_fib(n-2);
}                                   
                </programlisting>
                <para> We now need a recursive fibonacci task: </para>
                <programlisting>// our recursive fibonacci tasks. Needs to inherit continuation_task&lt;value type returned by this task>
struct fib_task : public boost::asynchronous::continuation_task&lt;long>
{
    fib_task(long n,long cutoff):n_(n),cutoff_(cutoff){}
    // called inside of threadpool
    void operator()()const
    {
        // the result of this task, will be either set directly if &lt; cutoff, otherwise when taks is ready
        boost::asynchronous::continuation_result&lt;long> task_res = this_task_result();
        if (n_&lt;cutoff_)
        {
            // n &lt; cutoff => execute immediately
            task_res.set_value(serial_fib(n_));
        }
        else
        {
            // n>= cutoff, create 2 new tasks and when both are done, set our result (res(task1) + res(task2))
            boost::asynchronous::create_continuation&lt;long>(
                        // called when subtasks are done, set our result
                        [task_res](std::tuple&lt;boost::future&lt;long>,boost::future&lt;long> >&amp;&amp; res)
                        {
                            long r = std::get&lt;0>(res).get() + std::get&lt;1>(res).get();
                            task_res.set_value(r);
                        },
                        // recursive tasks
                        fib_task(n_-1,cutoff_),
                        fib_task(n_-2,cutoff_));
        }
    }
    long n_;
    long cutoff_;
};                               
                </programlisting>
                <para> This deserves a bit of explanation. Our task need to inherit
                        <code>boost::asynchronous::continuation_task&lt;R></code> where R is the
                    type later returned. This class provides us with <code>this_task_result()</code>
                    where we set the task result. This is done either immediately if n &lt; cutoff
                    (first if clause), or (else clause) using a continuation.</para>
                <para>If n>= cutoff, we create a continuation task. This is a sleeping task, which
                    will get activated when all required tasks complete. In this case, we have two
                    fibonacci sub tasks. The template argument is the return type of the
                    continuation. We create two sub-tasks, for n-1 and n-2 and when they complete,
                    the completion functor passed as first argument is called.</para>
                <para>Note that <code>boost::asynchronous::create_continuation</code> is a variadic
                    function, there can be any number of sub-tasks. The completion functor takes as
                    single argument a tuple of futures, one for each subtask. The template argument
                    of the future is the template argument of
                        <code>boost::asynchronous::continuation_task</code> of each subtask. In this
                    case, all are long, but it's not a requirement.</para>
                <para>When this completion functor is called, we set our result to be result of
                    first task + result of second task and return.</para>
                <para>The main particularity of this solution is that a task does not block until
                    sub-tasks complete but instead provides an asynchronous functor.</para>
                <para>All what we still need to do is create the first task. In the tradition of
                    Asynchronous, we do it inside an asynchronous servant which posts the first task
                    and waits for a callback:</para>
                <programlisting>struct Servant : boost::asynchronous::trackable_servant&lt;>
{
...
   void calc_fibonacci(long n,long cutoff)
   {
      post_callback(
            [n,cutoff]()
            {
                // a top-level continuation is the first one in a recursive serie.
                // Its result will be passed to callback
                return boost::asynchronous::top_level_continuation&lt;long>(fib_task(n,cutoff));
            }// work
            ,
            // callback with fibonacci result.
            [](boost::future&lt;long> res){...}// callback functor.
        );                                 
   }  
   };          </programlisting>
                <para> We call <code>post_callback</code>, which, as usual, ensure that the callback
                    is posted to the right thread and the servant lifetime is tracked. The posted
                    task calls
                        <code>boost::asynchronous::top_level_continuation&lt;task-return-type></code>
                    to create the first, top-level continuation, passing it a first fib_task. This
                    is non-blocking, a special version of <code>post_callback</code> recognizes a
                    continuation and will call its callback (with a
                        <code>future&lt;task-return-type></code>) only when the calculation is
                    finished.</para>
                <para>As usual, calling get on the future is non-blocking, one gets eaither the
                    result or an exception if thrown by a task.</para>
                <para>Please have a look at the <link xlink:href="examples/example_fibonacci.cpp"
                        >complete example</link>.</para>
                <para>And what about logging? We don't want to give up this feature of course and
                    would like to know how long all these fib_task took to complete. This is done
                    through minor changes. As always we need a job, the same as usual:</para>
                <programlisting>typedef boost::asynchronous::any_loggable&lt;boost::chrono::high_resolution_clock> servant_job;                                 
                </programlisting>
                <para> We give the logged name of the task in the constructor of fib_task, for
                    example fib_task_xxx:</para>
                <programlisting>fib_task(long n,long cutoff)
        : boost::asynchronous::continuation_task&lt;long>("fib_task_" + boost::lexical_cast&lt;std::string>(n))
        ,n_(n),cutoff_(cutoff){}                                
                </programlisting>
                <para>And call <code>boost::asynchronous::create_continuation_log</code> instead of
                        <code>boost::asynchronous::create_continuation</code>:</para>
                <programlisting>boost::asynchronous::<emphasis role="bold">create_continuation_log</emphasis>&lt;long,servant_job>(
                        [task_res](std::tuple&lt;boost::future&lt;long>,boost::future&lt;long> >&amp;&amp; res)
                        {
                            long r = std::get&lt;0>(res).get() + std::get&lt;1>(res).get();
                            task_res.set_value(r);
                        },
                        fib_task(n_-1,cutoff_),
                        fib_task(n_-2,cutoff_)
);                               
                </programlisting>
                <para> Inside the servant we might optionally want the version of post_callback with
                    name, and we need to use <code>top_level_continuation_log</code> instead of
                        <code>top_level_continuation</code>:</para>
                <programlisting>post_callback(
              [n,cutoff]()
              {
                   return boost::asynchronous::<emphasis role="bold">top_level_continuation_log</emphasis>&lt;long,servant_job>(fib_task(n,cutoff));
              }// work
              ,
              // the lambda calls Servant, just to show that all is safe, Servant is alive if this is called
              [this](boost::future&lt;long> res){...},// callback functor.
              <emphasis role="bold">"calc_fibonacci"</emphasis>
        );                             
                </programlisting>
                <para> The previous example has been <link
                        xlink:href="examples/example_fibonacci_log.cpp">rewritten with logs and a
                        display of all tasks</link> (beware, with higher fibonacci numbers, this can
                    become a long list).. </para>
            </sect1>
        </chapter>
        <chapter>
            <title> In-depth usage. </title>
            <sect1>
                <title>Which protections you get, which ones you don't.</title>
                <para>todo</para>
            </sect1>
            <sect1>
                <title>Tips</title>
                <para>todo</para>
            </sect1>
        </chapter>
    </part>
    <part>
        <title>Reference</title>
        <chapter>
            <title>Common concepts</title>
            <subtitle>Subtitle of Chapter</subtitle>
            <sect1>
                <title>Section1 Title</title>
                <subtitle>Subtitle of Section 1</subtitle>
                <para>TODO</para>
            </sect1>
        </chapter>
        <chapter>
            <title>Queues</title>
            <para> Asynchronous provides a range of queues with different trade-offs. Use
                    <code>threadsafe_list</code> as default for a quickstart with
                Asynchronous.</para>
            <sect1>
                <title>threadsafe_list</title>
                <para>This queue is mostly the one presented in Anthony Williams' book, "C++
                    Concurrency In Action". It is made of a single linked list of nodes, with a
                    mutex at each end of the queue to minimize contention. It is reasonably fast and
                    of simple usage. It can be used in all configurations of pools. Please use this
                    container as default when starting with Asynchronous.</para>
                <para>Its constructor does not require any parameter forwarded from the
                    scheduler.</para>
                <para>Stealing: from the same queue end as pop. Will be implemented better (from the
                    other end to reduce contention) in a future version.</para>
                <para>Declaration:</para>
                <programlisting>template&lt;class JOB = boost::asynchronous::any_callable>
class threadsafe_list;                 
                </programlisting>
            </sect1>
            <sect1>
                <title>lockfree_queue</title>
                <para>This queue is a light wrapper around a <code>boost::lockfree::queue</code>,
                    which gives lockfree behavior at the cost of an extra dynamic memory allocation. </para>
                <para>The container is faster than a <code>threadsafe_list</code>, provided one
                    manages to set the queue size to an optimum value. A too small size will cause
                    expensive memory allocations, a too big size will significantly degrade
                    performance.</para>
                <para>Its constructor requires a default size forwarded from the scheduler.</para>
                <para>Stealing: from the same queue end as pop. Stealing from the other end is not
                    supported by <code>boost::lockfree::queue</code>. It can be used in all
                    configurations of pools.</para>
                <para>Declaration:</para>
                <programlisting>template&lt;class JOB = boost::asynchronous::any_callable>
class lockfree_queue;                 
                </programlisting>
            </sect1>
            <sect1>
                <title>lockfree_spsc_queue</title>
                <para>This queue is a light wrapper around a
                        <code>boost::lockfree::spsc_queue</code>, which gives lockfree behavior at
                    the cost of an extra dynamic memory allocation. </para>
                <para>Its constructor requires a default size forwarded from the scheduler.</para>
                <para>Stealing: None. Stealing is not supported by
                        <code>boost::lockfree::spsc_queue</code>. It can only be used
                    Single-Producer / Single-Consumer, which reduces its typical usage to a queue of
                    a <code>multiqueue_threadpool_scheduler</code> as consumer, with a
                        <code>single_thread_scheduler</code> as producer.</para>
                <para>Declaration:</para>
                <programlisting>template&lt;class JOB = boost::asynchronous::any_callable>
class lockfree_spsc_queue;                 
                </programlisting>
            </sect1>
            <sect1>
                <title>lockfree_stack</title>
                <para>This queue is a light wrapper around a <code>boost::lockfree::stack</code>,
                    which gives lockfree behavior at the cost of an extra dynamic memory allocation.
                    This container creates a task inversion as the last posted tasks will be
                    executed first.</para>
                <para>Its constructor requires a default size forwarded from the scheduler.</para>
                <para>Stealing: from the same queue end as pop. Stealing from the other end is not
                    supported by <code>boost::lockfree::stack</code>. It can be used in all
                    configurations of pools.</para>
                <para>Declaration:</para>
                <programlisting>template&lt;class JOB = boost::asynchronous::any_callable>
class lockfree_stack;                 
                </programlisting>
            </sect1>
        </chapter>
        <chapter>
            <title>Schedulers</title>
            <para> There is not the perfect scheduler. In any case it's a question of trade-off.
                Here are the schedulers offered by Asynchronous.</para>
            <sect1>
                <title>single_thread_scheduler</title>
                <para>The scheduler of choice for all servants which are not thread-safe. Serializes
                    all calls to a single queue and executes them in order. Using
                        <code>any_queue_container</code> as queue will however allow it to support
                    task priority.</para>
                <para>This scheduler does not steal from other queues or pools, only stealing_
                    threadpools do this, and does not get stolen from to avoid races.</para>
                <para>Declaration:</para>
                <programlisting>template&lt;class Queue>
class single_thread_scheduler;                 
                </programlisting>
                <para>Creation:</para>
                <programlisting>boost::asynchronous::any_shared_scheduler_proxy&lt;> scheduler = 
    boost::asynchronous::create_shared_scheduler_proxy(
                    new boost::asynchronous::single_thread_scheduler&lt;
                          boost::asynchronous::threadsafe_list&lt;> >);  

boost::asynchronous::any_shared_scheduler_proxy&lt;> scheduler = 
    boost::asynchronous::create_shared_scheduler_proxy(
                    new boost::asynchronous::single_thread_scheduler&lt;
                          boost::asynchronous::lockfree_queue&lt;> >(10)); // size of queue
                </programlisting>
                <para>Or, using logging:</para>
                <programlisting>typedef boost::asynchronous::any_loggable&lt;boost::chrono::high_resolution_clock> <emphasis role="bold">servant_job</emphasis>;

boost::asynchronous::any_shared_scheduler_proxy&lt;<emphasis role="bold">servant_job</emphasis>> scheduler = 
    boost::asynchronous::create_shared_scheduler_proxy(
                    new boost::asynchronous::single_thread_scheduler&lt;
                          boost::asynchronous::threadsafe_list&lt;<emphasis role="bold">servant_job</emphasis>> >);                                      
                
boost::asynchronous::any_shared_scheduler_proxy&lt;<emphasis role="bold">servant_job</emphasis>> scheduler = 
    boost::asynchronous::create_shared_scheduler_proxy(
                    new boost::asynchronous::single_thread_scheduler&lt;
                          boost::asynchronous::lockfree_queue&lt;<emphasis role="bold">servant_job</emphasis>> >(10)); // size of queue</programlisting>
                <para>
                    <table frame="all">
                        <title>#include
                            &lt;boost/asynchronous/scheduler/single_thread_scheduler.hpp></title>
                        <tgroup cols="2">
                            <colspec colname="c1" colnum="1" colwidth="1.0*"/>
                            <colspec colname="c2" colnum="2" colwidth="1.0*"/>
                            <thead>
                                <row>
                                    <entry>Characteristics</entry>
                                    <entry/>
                                </row>
                            </thead>
                            <tbody>
                                <row>
                                    <entry>Number of threads</entry>
                                    <entry>1</entry>
                                </row>
                                <row>
                                    <entry>Can be stolen from?</entry>
                                    <entry>No</entry>
                                </row>
                                <row>
                                    <entry>Can steal from other threads in this pool?</entry>
                                    <entry>N/A (only 1 thread)</entry>
                                </row>
                                <row>
                                    <entry>Can steal from other threads in other pools?</entry>
                                    <entry>No</entry>
                                </row>
                            </tbody>
                        </tgroup>
                    </table>
                </para>
            </sect1>
            <sect1>
                <title>threadpool_scheduler</title>
                <para>The simplest and easiest threadpool using a single queue, though multiqueue
                    behavior could be done using <code>any_queue_container</code>. The advantage is
                    that it allows the pool to be given 0 thread and only be stolen from. The cost
                    is a slight performance loss due to higher contention on the single
                    queue.</para>
                <para>This pool does not steal from other pool's queues.</para>
                <para>Use this pool as default for a quickstart with Asynchronous.</para>
                <para>Declaration:</para>
                <programlisting>template&lt;class Queue>
class threadpool_scheduler;                 
                </programlisting>
                <para>Creation:</para>
                <programlisting>boost::asynchronous::any_shared_scheduler_proxy&lt;> scheduler = 
    boost::asynchronous::create_shared_scheduler_proxy(
                    new boost::asynchronous::threadpool_scheduler&lt;
                          boost::asynchronous::threadsafe_list&lt;> >(4)); // 4 threads in pool  

boost::asynchronous::any_shared_scheduler_proxy&lt;> scheduler = 
    boost::asynchronous::create_shared_scheduler_proxy(
                    new boost::asynchronous::threadpool_scheduler&lt;
                          boost::asynchronous::lockfree_queue&lt;> >(4,10)); // size of queue=10, 4 threads in pool
                </programlisting>
                <para>Or, using logging:</para>
                <programlisting>typedef boost::asynchronous::any_loggable&lt;boost::chrono::high_resolution_clock> <emphasis role="bold">servant_job</emphasis>;

boost::asynchronous::any_shared_scheduler_proxy&lt;<emphasis role="bold">servant_job</emphasis>> scheduler = 
    boost::asynchronous::create_shared_scheduler_proxy(
                    new boost::asynchronous::threadpool_scheduler&lt;
                          boost::asynchronous::threadsafe_list&lt;<emphasis role="bold">servant_job</emphasis>> >(4)); // 4 threads in pool                                      
                
boost::asynchronous::any_shared_scheduler_proxy&lt;<emphasis role="bold">servant_job</emphasis>> scheduler = 
    boost::asynchronous::create_shared_scheduler_proxy(
                    new boost::asynchronous::threadpool_scheduler&lt;
                          boost::asynchronous::lockfree_queue&lt;<emphasis role="bold">servant_job</emphasis>> >(4,10)); // size of queue=10, 4 threads in pool  </programlisting>
                <para>
                    <table frame="all">
                        <title>#include
                            &lt;boost/asynchronous/scheduler/threadpool_scheduler.hpp></title>
                        <tgroup cols="2">
                            <colspec colname="c1" colnum="1" colwidth="1.0*"/>
                            <colspec colname="c2" colnum="2" colwidth="1.0*"/>
                            <thead>
                                <row>
                                    <entry>Characteristics</entry>
                                    <entry/>
                                </row>
                            </thead>
                            <tbody>
                                <row>
                                    <entry>Number of threads</entry>
                                    <entry><emphasis role="bold">0</emphasis>-n</entry>
                                </row>
                                <row>
                                    <entry>Can be stolen from?</entry>
                                    <entry>Yes</entry>
                                </row>
                                <row>
                                    <entry>Can steal from other threads in this pool?</entry>
                                    <entry>N/A (only 1 queue)</entry>
                                </row>
                                <row>
                                    <entry>Can steal from other threads in other pools?</entry>
                                    <entry>No</entry>
                                </row>
                            </tbody>
                        </tgroup>
                    </table>
                </para>
            </sect1>
            <sect1>
                <title>multiqueue_threadpool_scheduler</title>
                <para>This is a <code>threadpool_scheduler</code> with multiple queues to reduce
                    contention. On the other hand, this pool requires at least one thread.</para>
                <para>This pool does not steal from other pool's queues though pool threads do steal
                    from each other's queues.</para>
                <para>Declaration:</para>
                <programlisting>template&lt;class Queue,class FindPosition=boost::asynchronous::default_find_position&lt; > >
class multiqueue_threadpool_scheduler;                 
                </programlisting>
                <para>Creation:</para>
                <programlisting>boost::asynchronous::any_shared_scheduler_proxy&lt;> scheduler = 
    boost::asynchronous::create_shared_scheduler_proxy(
                    new boost::asynchronous::multiqueue_threadpool_scheduler&lt;
                          boost::asynchronous::threadsafe_list&lt;> >(4)); // 4 threads in pool  

boost::asynchronous::any_shared_scheduler_proxy&lt;> scheduler = 
    boost::asynchronous::create_shared_scheduler_proxy(
                    new boost::asynchronous::multiqueue_threadpool_scheduler&lt;
                          boost::asynchronous::lockfree_queue&lt;> >(4,10)); // size of queue=10, 4 threads in pool
                </programlisting>
                <para>Or, using logging:</para>
                <programlisting>typedef boost::asynchronous::any_loggable&lt;boost::chrono::high_resolution_clock> <emphasis role="bold">servant_job</emphasis>;

boost::asynchronous::any_shared_scheduler_proxy&lt;<emphasis role="bold">servant_job</emphasis>> scheduler = 
    boost::asynchronous::create_shared_scheduler_proxy(
                    new boost::asynchronous::multiqueue_threadpool_scheduler&lt;
                          boost::asynchronous::threadsafe_list&lt;<emphasis role="bold">servant_job</emphasis>> >(4)); // 4 threads in pool                                      
                
boost::asynchronous::any_shared_scheduler_proxy&lt;<emphasis role="bold">servant_job</emphasis>> scheduler = 
    boost::asynchronous::create_shared_scheduler_proxy(
                    new boost::asynchronous::multiqueue_threadpool_scheduler&lt;
                          boost::asynchronous::lockfree_queue&lt;<emphasis role="bold">servant_job</emphasis>> >(4,10)); // size of queue=10, 4 threads in pool  </programlisting>
                <para>
                    <table frame="all">
                        <title>#include
                            &lt;boost/asynchronous/scheduler/multiqueue_threadpool_scheduler.hpp></title>
                        <tgroup cols="2">
                            <colspec colname="c1" colnum="1" colwidth="1.0*"/>
                            <colspec colname="c2" colnum="2" colwidth="1.0*"/>
                            <thead>
                                <row>
                                    <entry>Characteristics</entry>
                                    <entry/>
                                </row>
                            </thead>
                            <tbody>
                                <row>
                                    <entry>Number of threads</entry>
                                    <entry><emphasis role="bold">1</emphasis>-n</entry>
                                </row>
                                <row>
                                    <entry>Can be stolen from?</entry>
                                    <entry>Yes</entry>
                                </row>
                                <row>
                                    <entry>Can steal from other threads in this pool?</entry>
                                    <entry>Yes</entry>
                                </row>
                                <row>
                                    <entry>Can steal from other threads in other pools?</entry>
                                    <entry>No</entry>
                                </row>
                            </tbody>
                        </tgroup>
                    </table>
                </para>
            </sect1>
            <sect1>
                <title>stealing_threadpool_scheduler</title>
                <para>This is a <code>threadpool_scheduler</code> with the added capability to steal
                    from other pool's queues within a <code>composite_threadpool_scheduler</code>.
                    Not used within a <code>composite_threadpool_scheduler</code>, it is a standard
                        <code>threadpool_scheduler</code>.</para>
                <para>Declaration:</para>
                <programlisting>template&lt;class Queue,bool /* InternalOnly */ = true >
class stealing_threadpool_scheduler;                 
                </programlisting>
                <para>Creation if used within a <code>composite_threadpool_scheduler</code>:</para>
                <programlisting>boost::asynchronous::any_shared_scheduler_proxy&lt;> scheduler = 
    boost::asynchronous::create_shared_scheduler_proxy(
                    new boost::asynchronous::stealing_threadpool_scheduler&lt;
                          boost::asynchronous::threadsafe_list&lt;> >(4)); // 4 threads in pool  
                </programlisting>
                <para> However, if used stand-alone, which has little interest outside of unit
                    tests, we need to add a template parameter to inform it:</para>
                <programlisting>boost::asynchronous::any_shared_scheduler_proxy&lt;> scheduler = 
    boost::asynchronous::create_shared_scheduler_proxy(
                    new boost::asynchronous::stealing_threadpool_scheduler&lt;
                          boost::asynchronous::threadsafe_list&lt;><emphasis role="bold">,true</emphasis> >(4)); // 4 threads in pool  
                </programlisting>
                <para>
                    <table frame="all">
                        <title>#include
                            &lt;boost/asynchronous/scheduler/stealing_threadpool_scheduler.hpp></title>
                        <tgroup cols="2">
                            <colspec colname="c1" colnum="1" colwidth="1.0*"/>
                            <colspec colname="c2" colnum="2" colwidth="1.0*"/>
                            <thead>
                                <row>
                                    <entry>Characteristics</entry>
                                    <entry/>
                                </row>
                            </thead>
                            <tbody>
                                <row>
                                    <entry>Number of threads</entry>
                                    <entry><emphasis role="bold">0</emphasis>-n</entry>
                                </row>
                                <row>
                                    <entry>Can be stolen from?</entry>
                                    <entry>Yes</entry>
                                </row>
                                <row>
                                    <entry>Can steal from other threads in this pool?</entry>
                                    <entry>N/A (only 1 queue)</entry>
                                </row>
                                <row>
                                    <entry>Can steal from other threads in other pools?</entry>
                                    <entry>Yes</entry>
                                </row>
                            </tbody>
                        </tgroup>
                    </table>
                </para>
            </sect1>
            <sect1>
                <title>stealing_multiqueue_threadpool_scheduler</title>
                <para>This is a <code>multiqueue_threadpool_scheduler</code> with the added
                    capability to steal from other pool's queues within a
                        <code>composite_threadpool_scheduler</code> (of course, threads within this
                    pool do steal from each other queues, with higher priority). Not used within a
                        <code>composite_threadpool_scheduler</code>, it is a standard
                        <code>multiqueue_threadpool_scheduler</code>.</para>
                <para>Declaration:</para>
                <programlisting>template&lt;class Queue,class FindPosition=boost::asynchronous::default_find_position&lt; >,bool /* InternalOnly */= true  >
class stealing_multiqueue_threadpool_scheduler;                 
                </programlisting>
                <para>Creation if used within a <code>composite_threadpool_scheduler</code>:</para>
                <programlisting>boost::asynchronous::any_shared_scheduler_proxy&lt;> scheduler = 
    boost::asynchronous::create_shared_scheduler_proxy(
                    new boost::asynchronous::stealing_multiqueue_threadpool_scheduler&lt;
                          boost::asynchronous::threadsafe_list&lt;> >(4)); // 4 threads in pool  
                </programlisting>
                <para> However, if used stand-alone, which has little interest outside of unit
                    tests, we need to add a template parameter to inform it:</para>
                <programlisting>boost::asynchronous::any_shared_scheduler_proxy&lt;> scheduler = 
    boost::asynchronous::create_shared_scheduler_proxy(
                    new boost::asynchronous::stealing_multiqueue_threadpool_scheduler&lt;
                          boost::asynchronous::threadsafe_list&lt;>,boost::asynchronous::default_find_position&lt;>,<emphasis role="bold">true</emphasis>  >(4)); // 4 threads in pool  
                </programlisting>
                <para>
                    <table frame="all">
                        <title>#include
                            &lt;boost/asynchronous/stealing_multiqueue_threadpool_scheduler.hpp></title>
                        <tgroup cols="2">
                            <colspec colname="c1" colnum="1" colwidth="1.0*"/>
                            <colspec colname="c2" colnum="2" colwidth="1.0*"/>
                            <thead>
                                <row>
                                    <entry>Characteristics</entry>
                                    <entry/>
                                </row>
                            </thead>
                            <tbody>
                                <row>
                                    <entry>Number of threads</entry>
                                    <entry><emphasis role="bold">1</emphasis>-n</entry>
                                </row>
                                <row>
                                    <entry>Can be stolen from?</entry>
                                    <entry>Yes</entry>
                                </row>
                                <row>
                                    <entry>Can steal from other threads in this pool?</entry>
                                    <entry>Yes</entry>
                                </row>
                                <row>
                                    <entry>Can steal from other threads in other pools?</entry>
                                    <entry>Yes</entry>
                                </row>
                            </tbody>
                        </tgroup>
                    </table>
                </para>
            </sect1>
            <sect1>
                <title>composite_threadpool_scheduler</title>
                <para>This pool has no thread by itself. Its job is to contain other pools,
                    accessible by the priority given by posting, and share all queues of its
                    subpools among them. Only the stealing_* pools and <code>asio_scheduler</code>
                    will make use of this and steal from other pools though.</para>
                <para>For creation we need to create other pool of stealing or not stealing, stolen
                    from or not, schedulers. stealing_xxx pools will try to steal jobs from other
                    pool of the same composite, but only if these schedulers support this. Other
                    threadpools will not steal but get stolen from.
                        <code>single_thread_scheduler</code> will not steal or get stolen
                    from.</para>
                <programlisting>// create a composite threadpool made of:
// a multiqueue_threadpool_scheduler, 0 thread
// This scheduler does not steal from other schedulers, but will lend its queues for stealing
auto tp = boost::asynchronous::create_shared_scheduler_proxy(
                    new boost::asynchronous::threadpool_scheduler&lt;boost::asynchronous::lockfree_queue&lt;> > (0,100));

// a stealing_multiqueue_threadpool_scheduler, 3 threads, each with a threadsafe_list
// this scheduler will steal from other schedulers if it can. In this case it will manage only with tp, not tp3
auto tp2 = boost::asynchronous::create_shared_scheduler_proxy(
                    new boost::asynchronous::stealing_multiqueue_threadpool_scheduler&lt;boost::asynchronous::threadsafe_list&lt;> > (3));

// composite pool made of the previous 2
auto tp_worker = boost::asynchronous::create_shared_scheduler_proxy(new <emphasis role="bold">boost::asynchronous::composite_threadpool_scheduler&lt;> (tp,tp2)</emphasis>); 
                </programlisting>
                <para>Declaration:</para>
                <programlisting>template&lt;class Job = boost::asynchronous::any_callable,
         class FindPosition=boost::asynchronous::default_find_position&lt; >,
         class Clock = boost::chrono::high_resolution_clock  >
class composite_threadpool_scheduler;                 
                </programlisting>
                <para>
                    <table frame="all">
                        <title>#include
                            &lt;boost/asynchronous/scheduler/composite_threadpool_scheduler.hpp></title>
                        <tgroup cols="2">
                            <colspec colname="c1" colnum="1" colwidth="1.0*"/>
                            <colspec colname="c2" colnum="2" colwidth="1.0*"/>
                            <thead>
                                <row>
                                    <entry>Characteristics</entry>
                                    <entry/>
                                </row>
                            </thead>
                            <tbody>
                                <row>
                                    <entry>Number of threads</entry>
                                    <entry>0</entry>
                                </row>
                                <row>
                                    <entry>Can be stolen from?</entry>
                                    <entry>Yes</entry>
                                </row>
                                <row>
                                    <entry>Can steal from other threads in this pool?</entry>
                                    <entry>N/A</entry>
                                </row>
                                <row>
                                    <entry>Can steal from other threads in other pools?</entry>
                                    <entry>No</entry>
                                </row>
                            </tbody>
                        </tgroup>
                    </table>
                </para>
            </sect1>
            <sect1>
                <title>asio_scheduler</title>
                <para>This pool brings the infrastructure and access to io_service for an integrated
                    usage of Boost.Asio. Furthermore, if used withing a
                        <code>composite_threadpool_scheduler</code>, it will steal jobs from other
                    pool's queues.</para>
                <para>Declaration:</para>
                <programlisting>template&lt;class FindPosition=boost::asynchronous::default_find_position&lt; boost::asynchronous::sequential_push_policy > >
class asio_scheduler;                 
                </programlisting>
                <para>Creation:</para>
                <programlisting>boost::asynchronous::any_shared_scheduler_proxy&lt;> scheduler = 
    boost::asynchronous::create_shared_scheduler_proxy(
                    new boost::asynchronous::asio_scheduler&lt;>(4)); // 4 threads in pool  
                </programlisting>
                <para>
                    <table frame="all">
                        <title>#include
                            &lt;boost/asynchronous/extensions/asio/asio_scheduler.hpp></title>
                        <tgroup cols="2">
                            <colspec colname="c1" colnum="1" colwidth="1.0*"/>
                            <colspec colname="c2" colnum="2" colwidth="1.0*"/>
                            <thead>
                                <row>
                                    <entry>Characteristics</entry>
                                    <entry/>
                                </row>
                            </thead>
                            <tbody>
                                <row>
                                    <entry>Number of threads</entry>
                                    <entry><emphasis role="bold">1</emphasis>-n</entry>
                                </row>
                                <row>
                                    <entry>Can be stolen from?</entry>
                                    <entry>No*</entry>
                                </row>
                                <row>
                                    <entry>Can steal from other threads in this pool?</entry>
                                    <entry>Yes</entry>
                                </row>
                                <row>
                                    <entry>Can steal from other threads in other pools?</entry>
                                    <entry>Yes</entry>
                                </row>
                            </tbody>
                        </tgroup>
                    </table>
                </para>
            </sect1>
        </chapter>
        <chapter>
            <title>Compiler</title>
            <sect1>
                <title>C++ 11</title>
                <para>Asynchronous is C++11-only. Please check that your compiler has C++11 enabled
                    (-std=c++0x or -std=c++11 in different versions of gcc)</para>
            </sect1>
            <sect1>
                <title>Supported compilers</title>
                <para>At the moment, Asynchronous has only be tested with gcc versions 4.7 to 4.8
                    and clang 3.3.</para>
            </sect1>
        </chapter>
    </part>
</book>
